{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNU6wpHo/OSFA/P3Lj0dmF+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikeogunmakin/research/blob/main/growth%20data%20science/202511_measuring_the_success_of_a_feature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Measuring the Success of a Feature**"
      ],
      "metadata": {
        "id": "1zXOw3OP4NJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Amplitude Framework"
      ],
      "metadata": {
        "id": "oZs8nFxQiQjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link: https://amplitude.com/blog/steps-measuring-feature"
      ],
      "metadata": {
        "id": "Muel-cGuiWUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Measure the Basic Usage of the feature\n",
        "2.   Dig deeper into event properties to look for patterns\n",
        "3. Understand what users are doing right before using the feature\n",
        "4. Build a behavioural cohort of people who used the feature to analyse how they compare to your overall user population\n",
        "5. Analyse the impact of the new feature on retention.\n",
        "6. Measure the impact of the new feature on your key conversion funnels\n",
        "7. Measure the impact of your feature on engagement.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LX4QY-dB8NQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Measure basic usage of the new feature"
      ],
      "metadata": {
        "id": "-tZ-xCjX_RTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Here you want to understnd whether people are using the feature. There are a few key metrics you should look at to get a complete answer to that question:\n",
        "\n",
        "* total number of times people are using the feature\n",
        "* the number of unique users who are using the feature\n",
        "* the percentage of your total active users who are using the feature\n",
        "* the average number of times per day users are using the feature\n",
        "\n",
        "\n",
        "It's useful to see how metrics trend over time."
      ],
      "metadata": {
        "id": "ZaquFeTt_YTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Dig deeper into event properties to look for patterns"
      ],
      "metadata": {
        "id": "TmLpFpD_AHKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is about understanding HOW people are using the new feature. Consider behavioural segments relevant to the feature. For example, if ythe new feature allows users to share music, you could explore what type of music is commonly shared. Also consider whether user segment could potentially be worth exploring."
      ],
      "metadata": {
        "id": "CeeLJn-oAMe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Understand what users are doing right before using the feature"
      ],
      "metadata": {
        "id": "LbYAq7uIBpL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It’s helpful to trace the paths that users take before they use a feature. What are most users doing right before they use a feature? This gives you an idea of the context in which they’re are using a new feature.\n",
        "\n",
        "\n",
        "\n",
        "E.g. About 40% of users who ShareMusic play a song and then share music, implying that they listen to a song they like, and immediately share that song with their friends. Also, 30% of users share music right after favoriting a song.\n",
        "\n",
        "\n",
        "You can apply what you learn and give recommendations. If someone is marking a song as a favorite, that’s probably a good time to prompt them share that song with their friends, since they clearly are finding value in that song. You can test whether adding a small in-app prompt right after someone favorites a song saying ‘Share this song with your friends!’ will increase the number of users who share, as well as total shares in your app."
      ],
      "metadata": {
        "id": "-fimVDnUC4Il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Build a behavioral cohort of people who used the feature to analyze how they compare to your overall user population"
      ],
      "metadata": {
        "id": "Q99hK_QXHH8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple cohort to show users who used the feature vs those that dont"
      ],
      "metadata": {
        "id": "rL9nPbM6HJxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Analyze the impact of the new feature on retention"
      ],
      "metadata": {
        "id": "fikN42vSHl2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To measure the  impact of feature on retention, you can compare the retention of the cohort of users who used the feature vs. those who did not.\n",
        "\n",
        "One way to do this is to look specifically at new user retention: do new users who use the feature early in their experience (like their first day), retain better than new users who don’t?\n",
        "\n",
        "You could analyse using retention curve. E.g \"The retention curve above shows that new users who share music on their first day in the app have significantly higher retention by Day 30 than new users who don’t share music. About 40% of people who shared music returned to the app 30 days later, compared to only 5% of non-sharers.\""
      ],
      "metadata": {
        "id": "HMCvHqvnIAMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Measure the impact of the new feature on your key conversion funnels"
      ],
      "metadata": {
        "id": "yX2CG6CTJ0Mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could compare users that use the features vs those that do not on conversion funnel."
      ],
      "metadata": {
        "id": "od_abMWJJ-UE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f9sS0QhEiw5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Measure the impact of your new feature on engagement"
      ],
      "metadata": {
        "id": "_L5EtNHrOLCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you’ve seen the impact sharing has on conversion and retention, you’re curious whether users who share music are more engaged with your mobile app overall. One way to look at this is the number of sessions each user does per day, split by cohort"
      ],
      "metadata": {
        "id": "7tp77JGGOP-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paul Levchuck's Framework"
      ],
      "metadata": {
        "id": "rQxPcwLmity9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link: https://medium.com/@paul.levchuk/how-do-you-measure-the-success-of-a-product-feature-28240cf20aa3\n"
      ],
      "metadata": {
        "id": "LgLnhP_IoZ9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. how popular is this feature among Weekly/Daily Active Users (popularity)?\n",
        "2. how regularly are users using this feature per week (habit)?\n",
        "3. how often are users using this feature per day (usage)?\n",
        "4. what is the impact on retention rate (value)?\n",
        "   - An enhanced version of the analysis of how product features impact retention\n",
        "   - Two types of feature retention\n",
        "   - Product feature retention deep dive — Information gain\n",
        "   - Product feature retention deep dive — MCC coefficient\n",
        "5. what is the difference in product feature usage between free and paid users? (monetization)\n",
        "6. what users are doing before using this feature and after it (context)?\n",
        "7. what is the completion rate of using this feature (simplicity)?"
      ],
      "metadata": {
        "id": "tkMoOxImi02F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Popularity"
      ],
      "metadata": {
        "id": "EsHYOT_V7Jcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "link: https://medium.com/@paul.levchuk/how-popular-is-this-feature-among-weekly-daily-active-users-283a43d4cc3a"
      ],
      "metadata": {
        "id": "NAIhUNSdoipz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weekly tracking is usually the most effective period for measuring feature popularity. It provides enough data for stable insights while also reflecting the natural weekly seasonality seen in many products. Monthly tracking works better for long-term views, while daily tracking is helpful for immediate feedback but tends to be noisy.\n",
        "\n",
        "A typical analysis begins by looking at the number of events triggered by the feature. While this metric shows how often the feature is used, it can be misleading. A spike in event count may reflect a handful of heavy users or even a bug generating duplicate events. Since the number of events is just the number of users multiplied by the number of events per user, it does not necessarily represent an increase in actual popularity.\n",
        "\n",
        "A more meaningful starting point is the number of unique users using the feature. This gives a better sense of reach, but even this metric is influenced by changes in the overall size of the user base. During periods of increased user acquisition, the number of feature users can rise simply because there are more people in the product, not because the feature has become more popular.\n",
        "\n",
        "To understand real adoption, it is necessary to move to relative metrics such as the percentage of total active users who use the feature. This normalises growth and shows whether the feature is becoming more popular among the user base, regardless of how large the user base is.\n",
        "\n",
        "User segmentation provides even deeper insight. New users and old users often behave differently, and high acquisition periods can hide these differences. By breaking users into new and old cohorts, it becomes clear how each group responds to feature changes. For example, new users may respond strongly to increased feature visibility, while old users show no change at all. This may indicate that the feature is inherently more valuable to newcomers, or that existing users have already formed habits that are hard to change.\n",
        "\n",
        "Trend analysis should also account for weekly seasonality—regular patterns in user activity that repeat each week. Valid conclusions come from observing whether the percentage of users engaging with the feature is increasing over time once seasonality is considered.\n",
        "\n",
        "In summary, measuring feature popularity requires moving beyond raw event counts towards user-level and relative metrics, examined over weekly periods and segmented by user cohort. This approach reveals true behavioural patterns and avoids the misleading signals that absolute metrics often produce.\n"
      ],
      "metadata": {
        "id": "zDz9Rjdr7klC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Habit"
      ],
      "metadata": {
        "id": "BPYRDl7CC2HU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "link: https://medium.com/@paul.levchuk/how-regular-are-users-using-this-feature-per-week-133e2b80bfa0"
      ],
      "metadata": {
        "id": "fxPckXukmEy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When building a product, it’s not enough for lots of people to try a feature — **you want them to use it repeatedly**. That’s what tells you the feature is becoming a *habit*.\n",
        "\n",
        "If you simply calculate the *average number of days per user*, you compress everyone’s behaviour into one number.\n",
        "But this hides important patterns.\n",
        "\n",
        "Example:\n",
        "If nearly all users only use the feature once per week, the average will be low — but you won’t see how *extreme* (skewed) that pattern is.\n",
        "\n",
        "So instead of an average, we should look at a **distribution**:\n",
        "\n",
        "* What % used the feature 1 day?\n",
        "* What % used it 2 days?\n",
        "* … up to 7 days?\n",
        "\n",
        "This tells a much clearer story.\n"
      ],
      "metadata": {
        "id": "COpJa8pEIZR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Usage"
      ],
      "metadata": {
        "id": "lfbJmHLGQPd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "link: https://medium.com/@paul.levchuk/how-often-are-users-using-this-feature-per-day-e6a008bd06ca"
      ],
      "metadata": {
        "id": "bCZZnGhLmHEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Daily intensity usage is an important way to understand how deeply users engage with a product feature on a given day. While habit analysis tells us how often users return, intensity analysis focuses on how much value they extract each time they use the product. Some users only need to use a feature once or twice a day to receive value, while others—often those with more specific or heavier needs—may use it many times. This difference helps reveal what “casual” usage looks like and whether a subset of users gains exceptional value from the feature.\n",
        "\n",
        "A key principle in this analysis is to track the relationship between total events and total users. Since the number of events equals the number of users multiplied by the number of events per user, patterns in growth can signal changes in behaviour. If the number of users is growing faster than the number of events, it implies that the average events per user are decreasing—often because new, less intensive users are joining. This is why analysing the metric [# events per user] is essential.\n",
        "\n",
        "However, the first rule of thumb is to always segment daily intensity usage by new versus old users. These groups behave differently: old users are typically more experienced and therefore more intensive, while new users are still forming habits. In the example, new users showed a peak intensity of around 3.7 uses per day, while old users reached approximately 5.6. The overall trend aligns more closely with new users because they form the majority. Still, both groups include users who use the feature extremely heavily—up to 50 times a day—highlighting the presence of core or high-need users.\n",
        "\n",
        "The second rule is to build histograms, as they reveal the distribution of daily usage rather than just the average. Histograms showed that 58.5% of new users used the feature only once per day, compared to 36.9% of old users. Meanwhile, 16.6% of new users and 37.8% of old users used the feature more than three times per day. These distributions help identify what typical usage looks like and how large the heavy-user segment is.\n",
        "\n",
        "In summary, daily intensity usage is a vital signal for understanding the value users derive from a feature. By segmenting new and old users and examining the full distribution through histograms, product teams can gain deeper insight into whether changes improve usage depth and whether the core, high-value user base is growing. This analysis becomes especially useful when tracking the impact of feature changes or design updates over time."
      ],
      "metadata": {
        "id": "ADEFBIytQUFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Value"
      ],
      "metadata": {
        "id": "d6l2qFIkpQuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "link: https://medium.com/@paul.levchuk/what-is-the-impact-on-the-retention-rate-4add415ecc24"
      ],
      "metadata": {
        "id": "BS1nq4oBpdcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2kifLeKtpiRc"
      }
    }
  ]
}